"""
Dataset Loader: S·ª≠ d·ª•ng IterableDataset ƒë·ªÉ b·∫Øt ƒë·∫ßu train NGAY L·∫¨P T·ª®C.
Kh√¥ng c·∫ßn ch·ªù ƒë·ª£i t·∫°o 1.1 tri·ªáu samples, d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c load theo ki·ªÉu "v·ª´a train v·ª´a load".
"""

from __future__ import annotations

import json
from pathlib import Path
from typing import Any, Optional

import yaml
from datasets import load_dataset, IterableDataset
from PIL import Image


def load_processed_dataset(
    config_path: str | Path,
    split: Optional[str] = None,
) -> dict[str, Any]:
    """Load metadata v√† tr·∫£ v·ªÅ IterableDataset ƒë·ªÉ train ngay."""
    config_path = Path(config_path)
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)

    data_cfg = config["data"]
    processed_dir = Path(data_cfg.get("processed_dir", "data/processed"))
    dataset_name = data_cfg["dataset_name"]
    image_resize = data_cfg.get("image_resize", 512)

    # 1. Load Original Dataset (Image Cache)
    print(f"üì¶ Connecting to original image cache: {dataset_name}")
    raw_images = load_dataset(dataset_name, split="train")

    # 2. Generator function
    def _gen_fn(meta_file: Path):
        with open(meta_file, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line)
                img_idx = item["idx"]
                qa_list = item["qa"]
                # Resize ·∫£nh: B·∫Øt bu·ªôc ph·∫£i l√† b·ªôi s·ªë c·ªßa 28 cho Qwen-VL
                img = raw_images[img_idx]["image"]
                if image_resize > 0:
                    w, h = img.size
                    scale = image_resize / max(w, h)
                    # L√†m tr√≤n v·ªÅ b·ªôi s·ªë c·ªßa 28 g·∫ßn nh·∫•t
                    new_w = max(28, (int(w * scale) // 28) * 28)
                    new_h = max(28, (int(h * scale) // 28) * 28)
                    
                    if new_w != w or new_h != h:
                        img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)

                # Format Unsloth
                messages = []
                for j, qa in enumerate(qa_list):
                    user_content = [{"type": "text", "text": qa["u"]}]
                    if j == 0:
                        user_content.append({"type": "image", "image": img})
                    messages.append({"role": "user", "content": user_content})
                    messages.append({"role": "assistant", "content": [{"type": "text", "text": qa["a"]}]})
                
                yield {"messages": messages}

    # 3. Tr·∫£ v·ªÅ IterableDataset (Kh√¥ng t·ªën th·ªùi gian generate tr∆∞·ªõc)
    datasets = {}
    if split in [None, "train"]:
        datasets["train"] = IterableDataset.from_generator(
            _gen_fn, gen_kwargs={"meta_file": processed_dir / "train_meta.jsonl"}
        )
    if split in [None, "val"]:
        datasets["val"] = IterableDataset.from_generator(
            _gen_fn, gen_kwargs={"meta_file": processed_dir / "val_meta.jsonl"}
        )
        
    return datasets
