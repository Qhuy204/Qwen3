# ============================================================
# Qwen3-VL-8B Fine-tuning Config
# Train: A100 80GB  |  Inference: RTX 3060 12GB
# ============================================================

model:
  name: "unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit"
  load_in_4bit: true
  max_seq_length: 2048
  use_gradient_checkpointing: "unsloth"

lora:
  r: 16
  lora_alpha: 16
  lora_dropout: 0
  bias: "none"
  random_state: 3407
  use_rslora: false
  finetune_vision_layers: true
  finetune_language_layers: true
  finetune_attention_modules: true
  finetune_mlp_modules: true

training:
  per_device_train_batch_size: 16     # Giảm xuống 16 để an toàn VRAM khi ảnh res cao
  gradient_accumulation_steps: 2       # Tăng lên 2 để giữ eff_batch = 32
  dataloader_num_workers: 8            # Dùng 8 nhân CPU để load/resize ảnh song song
  learning_rate: 2e-4                  # Giảm LR xuống 2e-4 theo khuyến nghị (tránh spike/overfit)
  max_grad_norm: 1.0                  # Thêm gradient clipping để ổn định training
  warmup_steps: 50
  # max_steps: 500                     # Uncomment nếu chỉ muốn test nhanh
  num_train_epochs: 1                  # 1 epoch đủ cho dataset lớn (~180k hybrid samples)
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  logging_steps: 5
  save_steps: 200                      # Tăng lên chút vì dataset lớn
  eval_steps: 200
  seed: 3407
  optim: "adamw_8bit"
  output_dir: "outputs"
  max_length: 2048
  fp16: false
  bf16: true                           # A100 supports bf16

data:
  dataset_name: "Qhuy204/VQA_VN_Destination"
  processed_dir: "data/processed"                # Thư mục lưu dataset đã xử lý
  train_ratio: 0.90                              # Tăng tập train
  val_ratio: 0.05
  test_ratio: 0.05
  max_qa_per_image: 10                           # Hybrid Strategy: 5 single + 5 consecutive pairs
  seed: 42

export:
  quantization_method: "q4_k_m"        # Tối ưu cho RTX 3060
  output_dir: "exported_model"
  merge_16bit_dir: "merged_model_16bit"
