# ============================================================
# Qwen3-VL-8B Fine-tuning Config
# Train: A100 80GB  |  Inference: RTX 3060 12GB
# ============================================================

model:
  name: "unsloth/Qwen3-VL-8B-Instruct-unsloth-bnb-4bit"
  load_in_4bit: true
  max_seq_length: 2048
  use_gradient_checkpointing: "unsloth"

lora:
  r: 16
  lora_alpha: 16
  lora_dropout: 0
  bias: "none"
  random_state: 3407
  use_rslora: false
  finetune_vision_layers: true
  finetune_language_layers: true
  finetune_attention_modules: true
  finetune_mlp_modules: true

training:
  per_device_train_batch_size: 32     # A100 80GB → tăng lên 32 cho máu
  gradient_accumulation_steps: 1       # Tăng batch rồi thì giảm accum về 1
  dataloader_num_workers: 8            # Dùng 8 nhân CPU để load/resize ảnh song song
  learning_rate: 4e-4                  # Tăng batch size thì nên tăng nhẹ LR
  warmup_steps: 40
  # max_steps: 500                     # Uncomment nếu chỉ muốn test nhanh
  num_train_epochs: 1                  # 1 epoch đủ cho dataset lớn (~1.16M samples)
  weight_decay: 0.01
  lr_scheduler_type: "cosine"
  logging_steps: 5
  save_steps: 100
  eval_steps: 100
  seed: 3407
  optim: "adamw_8bit"
  output_dir: "outputs"
  max_length: 2048
  fp16: false
  bf16: true                           # A100 supports bf16

data:
  dataset_name: "Qhuy204/VQA_VN_Destination"
  processed_dir: "data/processed"                # Thư mục lưu dataset đã xử lý
  train_ratio: 0.85
  val_ratio: 0.10
  test_ratio: 0.05
  max_qa_per_image: 10     # Lấy mẫu ngẫu nhiên 10 QA mỗi ảnh nếu vượt quá 10
  seed: 42

export:
  quantization_method: "q4_k_m"        # Tối ưu cho RTX 3060
  output_dir: "exported_model"
  merge_16bit_dir: "merged_model_16bit"
